{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Wwp02YxPZ3P"
   },
   "source": [
    "# Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from skmultiflow.data import FileStream\n",
    "from skmultiflow.neural_networks import PerceptronMask\n",
    "\n",
    "from skfeature.function.similarity_based import lap_score\n",
    "from skfeature.function.sparse_learning_based import MCFS\n",
    "from skfeature.utility.construct_W import construct_W\n",
    "\n",
    "import csv\n",
    "from csv import DictWriter\n",
    "import os\n",
    "\n",
    "from warnings import warn\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "np.seterr(all='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OG3zECvNLfuF"
   },
   "outputs": [],
   "source": [
    "class FEATURE_WEIGHTING:\n",
    "    def __init__(self, n_total_ftr, n_selected_features, target_values, model, mu_init=0, sigma_init=1, penalty_s=0.01, penalty_r=0.01, epochs=1, lr_mu=0.01, lr_sigma=0.01, scale_weights=True):\n",
    "        \"\"\"\n",
    "        :param n_total_ftr: (int) Total no. of features\n",
    "        :param n_selected_ftr: (int) The no. of features to select \n",
    "        :param target_values: (np.ndarray) Unique target values (class labels)\n",
    "        :param mu_init: (int/np.ndarray) Initial importance parameter\n",
    "        :param sigma_init: (int/np.ndarray) Initial uncertainty parameter\n",
    "        :param penalty_s: (float) Penalty factor for the uncertainty (corresponds to gamma_s in the paper)\n",
    "        :param penalty_r: (float) Penalty factor for the regularization (corresponds to gamma_r in the paper)\n",
    "        :param epochs: (int) No. of epochs that we use each batch of observations to update the parameters\n",
    "        :param lr_mu: (float) Learning rate for the gradient update of the importance\n",
    "        :param lr_sigma: (float) Learning rate for the gradient update of the uncertainty\n",
    "        :param scale_weights: (bool) If True, scale feature weights into the range [0,1]\n",
    "        :param model: (str) Name of the feature weighting model\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_total_ftr = n_total_ftr\n",
    "        self.n_selected_ftr = n_selected_features\n",
    "        self.target_values = target_values\n",
    "        self.mu = np.ones(n_total_ftr) * mu_init\n",
    "        self.sigma = np.ones(n_total_ftr) * sigma_init\n",
    "        self.var = np.ones(n_total_ftr) * mu_init\n",
    "        self.lap = np.ones(n_total_ftr) * mu_init\n",
    "        self.mcfs = np.ones(n_total_ftr) * mu_init\n",
    "        self.penalty_s = penalty_s\n",
    "        self.penalty_r = penalty_r\n",
    "        self.epochs = epochs\n",
    "        self.lr_mu = lr_mu\n",
    "        self.lr_sigma = lr_sigma\n",
    "        self.scale_weights = scale_weights\n",
    "        self.model = model\n",
    "        self.weights = np.ones(n_total_ftr) * 0\n",
    "        \n",
    "\n",
    "        # Additional model-specific parameters\n",
    "        self.model_param = {}\n",
    "        \n",
    "        # FIRES model\n",
    "        if self.model == 'fires' and tuple(target_values) != (-1, 1):\n",
    "            if len(np.unique(target_values)) == 2:\n",
    "                self.model_param['fires'] = True  # Indicates that we need to encode the target variable into {-1,1}\n",
    "                warn('FIRES WARNING: The target variable will be encoded as: {} = -1, {} = 1'.format(self.target_values[0], self.target_values[1]))\n",
    "            else:\n",
    "                raise ValueError('The target variable y must be binary.')\n",
    "                    \n",
    "    def weigh_features(self, x, y):\n",
    "        \"\"\"\n",
    "        Compute feature weights, given a batch of observations and corresponding labels\n",
    "        :param x: (np.ndarray) Batch of observations\n",
    "        :param y: (np.ndarray) Batch of labels\n",
    "        :return: feature weights\n",
    "        :rtype np.ndarray\n",
    "        \"\"\"\n",
    "\n",
    "        # Update estimates of mu and sigma given the predictive model\n",
    "        if self.model == 'fires':\n",
    "            self.__fires(x, y)\n",
    "            \n",
    "        elif self.model == 'standard':\n",
    "            self.__standard(x, y)\n",
    "            \n",
    "        elif self.model == 'max variance':\n",
    "            self.__max_variance(x, y)\n",
    "         \n",
    "        elif self.model == 'laplacian':\n",
    "            self.__laplacian_score(x, y)\n",
    "            \n",
    "        elif self.model == 'mcfs':\n",
    "            self.__mcfs_score(x, y)\n",
    "            \n",
    "        elif self.model == 'offesel':\n",
    "            self.__offesel(x, y)\n",
    "            \n",
    "        else:\n",
    "            raise NotImplementedError('The given model name does not exist')\n",
    "\n",
    "        # Limit sigma to range [0, inf]\n",
    "        if sum(n < 0 for n in self.sigma) > 0:\n",
    "            self.sigma[self.sigma < 0] = 0\n",
    "            warn('Sigma has automatically been rescaled to [0, inf], because it contained negative values.')\n",
    "\n",
    "        # Compute feature weights\n",
    "        return self.__compute_weights()\n",
    "\n",
    "    def __fires(self, x, y):\n",
    "        \"\"\"\n",
    "        Update the distribution parameters mu and sigma by optimizing them in terms of the (log) likelihood.\n",
    "        Here we assume a Bernoulli distributed target variable. We use a Probit model as our base model.\n",
    "        This corresponds to the FIRES-GLM model in the paper.\n",
    "        :param x: (np.ndarray) Batch of observations (numeric values only, consider normalizing data for better results)\n",
    "        :param y: (np.ndarray) Batch of labels: type binary, i.e. {-1,1} (bool, int or str will be encoded accordingly)\n",
    "        \"\"\"\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            # Encode target as {-1,1}\n",
    "            if 'fires' in self.model_param:\n",
    "                y[y == self.target_values[0]] = -1\n",
    "                y[y == self.target_values[1]] = 1\n",
    "                \n",
    "            # Shuffle the observations\n",
    "            random_idx = np.random.permutation(len(y))\n",
    "            x = x[random_idx]\n",
    "            y = y[random_idx]\n",
    "\n",
    "            # Iterative update of mu and sigma\n",
    "            try:\n",
    "                # Helper functions\n",
    "                dot_mu_x = np.dot(x, self.mu)\n",
    "                rho = np.sqrt(1 + np.dot(x**2, self.sigma**2))\n",
    "\n",
    "                # Gradients\n",
    "                nabla_mu = norm.pdf(y/rho * dot_mu_x) * (y/rho * x.T)\n",
    "                nabla_sigma = norm.pdf(y/rho * dot_mu_x) * (- y/(2 * rho**3) * 2 * (x**2 * self.sigma).T * dot_mu_x)\n",
    "\n",
    "                # Marginal Likelihood\n",
    "                marginal = norm.cdf(y/rho * dot_mu_x)\n",
    "\n",
    "                # Update parameters\n",
    "                self.mu += self.lr_mu * np.mean(nabla_mu / marginal, axis=1)\n",
    "                self.sigma += self.lr_sigma * np.mean(nabla_sigma / marginal, axis=1)\n",
    "                    \n",
    "            except TypeError as e:\n",
    "                raise TypeError('All features must be a numeric data type.') from e\n",
    "                \n",
    "    def __max_variance(self, x, y):\n",
    "        \"\"\" \n",
    "        Update the max variance using learning rate.\n",
    "        Here we calculate the distribution parameters mu and sigma using standard calculation\n",
    "        \n",
    "        :param x: (np.ndarray) Batch of observations\n",
    "        :param y: (np.ndarray) Batch of labels: type binary, i.e. {-1,1} (bool, int or str will be encoded accordingly)\n",
    "        \"\"\"\n",
    "        # Iterative update\n",
    "        try:\n",
    "            standardVar = np.var(x, axis = 0)  \n",
    "            self.var += self.lr_mu * standardVar\n",
    " \n",
    "        except TypeError as e:\n",
    "            raise TypeError('All features must be a numeric data type.') from e\n",
    "\n",
    "    def __laplacian_score(self, x, y):\n",
    "        \"\"\" \n",
    "        Update the laplacian score using learning rate.\n",
    "        \n",
    "        :param x: (np.ndarray) Batch of observations\n",
    "        :param y: (np.ndarray) Batch of labels: type binary\n",
    "        \"\"\"\n",
    "        # Iterative update\n",
    "        try:\n",
    "            standardLap = lap_score.lap_score(X=x, W=construct_W(x))  \n",
    "            self.lap += self.lr_mu * standardLap\n",
    "                \n",
    "        except TypeError as e:\n",
    "            raise TypeError('All features must be a numeric data type.') from e\n",
    "\n",
    "    def __mcfs_score(self, x, y):\n",
    "        \"\"\" \n",
    "        Update the MCFS score using learning rate.\n",
    "        \n",
    "        :param x: (np.ndarray) Batch of observations\n",
    "        :param y: (np.ndarray) Batch of labels: type binary\n",
    "        \"\"\"\n",
    "        # Iterative update\n",
    "        try:\n",
    "            W = MCFS.mcfs(X=x, n_selected_features=self.n_selected_ftr, mode=\"raw\") \n",
    "            mcfs_score = W.max(1)\n",
    "                \n",
    "            self.mcfs += self.lr_mu * mcfs_score\n",
    "                \n",
    "        except TypeError as e:\n",
    "            raise TypeError('All features must be a numeric data type.') from e\n",
    "    \n",
    "    def __offesel(self, x, y):\n",
    "        \"\"\" \n",
    "        Update the max mean using learning rate.\n",
    "        \n",
    "        :param x: (np.ndarray) Batch of observations\n",
    "        :param y: (np.ndarray) Batch of labels: type binary\n",
    "        \"\"\"\n",
    "        # Iterative update \n",
    "        try:\n",
    "            standardMean = np.mean(x, axis = 0) \n",
    "            self.mu += self.lr_mu * standardMean\n",
    "    \n",
    "        except TypeError as e:\n",
    "            raise TypeError('All features must be a numeric data type.') from e\n",
    "                \n",
    "\n",
    "    def __compute_weights(self):\n",
    "        \"\"\"\n",
    "        Compute optimal weights according to the feature weighting model.\n",
    "        :return: feature weights\n",
    "        :rtype np.ndarray\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute optimal weights\n",
    "        if self.model == 'fires':\n",
    "            weights = (self.mu**2 - self.penalty_s * self.sigma**2) / (2 * self.penalty_r)\n",
    "            \n",
    "        elif self.model == 'max variance':\n",
    "            weights = self.var\n",
    "         \n",
    "        elif self.model == 'laplacian':\n",
    "            weights = self.lap\n",
    "            \n",
    "        elif self.model == 'mcfs':\n",
    "            weights = self.mcfs\n",
    "            \n",
    "        elif self.model == 'offesel':\n",
    "            weights = self.mu\n",
    "        \n",
    "        if self.scale_weights:  # Scale weights to [0,1]\n",
    "            weights = MinMaxScaler().fit_transform(weights.reshape(-1, 1)).flatten()\n",
    "\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4c_CbWtDOUdH"
   },
   "outputs": [],
   "source": [
    "def apply_experiment(dataset_path, dataset_name, tgt_index, batch_sizes, fractions, classifier, model, epochs=1):\n",
    "    \n",
    "    final_acc, final_run_time, final_selection_time, final_classification_time = [], [], [], []\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        \n",
    "        if model != 'without fs':\n",
    "            selected_ftr_lst = fractions\n",
    "        else:\n",
    "            selected_ftr_lst = [1]\n",
    "        \n",
    "        for selected_ftr in selected_ftr_lst:\n",
    "            \n",
    "            acc, fts_time, clf_time = [], [], []\n",
    "            \n",
    "            # Load data as scikit-multiflow FileStream\n",
    "            # NOTE: The models accepts only numeric values. Please one-hot-encode or factorize string/char variables\n",
    "            # Additionally, we suggest users to normalize all features, e.g. by using scikit-learn's MinMaxScaler()\n",
    "            stream = FileStream(dataset_path, target_idx=tgt_index)\n",
    "            stream.prepare_for_use()\n",
    "\n",
    "            # Initial fit of the predictive model\n",
    "            predictor = PerceptronMask()\n",
    "            n_selected_ftr = round(selected_ftr*stream.n_features)\n",
    "            target_values = stream.target_values\n",
    "\n",
    "            x, y = stream.next_sample(batch_size=batch_size)\n",
    "                \n",
    "            if model != 'without fs':\n",
    "                start_time_classification = time.time()\n",
    "                predictor.partial_fit(x, y, target_values)\n",
    "                clf_time.append(time.time() - start_time_classification)\n",
    "                \n",
    "                # Initialize the feature weighting model\n",
    "                fires_model = FEATURE_WEIGHTING(\n",
    "                              n_total_ftr=stream.n_features,          # Total no. of features\n",
    "                              n_selected_features = n_selected_ftr,   # no. of selected features\n",
    "                              target_values=target_values,            # Unique target values (class labels)\n",
    "                              mu_init=0,                              # Initial importance parameter\n",
    "                              sigma_init=1,                           # Initial uncertainty parameter\n",
    "                              penalty_s=0.01,                         # Penalty factor for the uncertainty (corresponds to gamma_s in the paper)\n",
    "                              penalty_r=0.01,                         # Penalty factor for the regularization (corresponds to gamma_r in the paper)\n",
    "                              epochs=epochs,                          # No. of epochs that we use each batch of observations to update the parameters\n",
    "                              lr_mu=0.01,                             # Learning rate for the gradient update of the importance\n",
    "                              lr_sigma=0.01,                          # Learning rate for the gradient update of the uncertainty\n",
    "                              scale_weights=True,                     # If True, scale feature weights into the range [0,1]\n",
    "                              model=model)                            # Name of the feature weighting model\n",
    "            else:\n",
    "                start_time_classification = time.time()\n",
    "                predictor.partial_fit(x, y, target_values)\n",
    "                clf_time.append(time.time() - start_time_classification)\n",
    "   \n",
    "            while stream.has_more_samples():\n",
    "                # Load a new sample\n",
    "                x, y = stream.next_sample(batch_size=batch_size)\n",
    "                \n",
    "                # Normalize using MinMaxScaler\n",
    "                scaler = MinMaxScaler()\n",
    "                x = scaler.fit_transform(x)\n",
    "                \n",
    "                if model != 'without fs':\n",
    "                \n",
    "                    # Select features\n",
    "                    start_time_selection = time.time()\n",
    "                    ftr_weights = fires_model.weigh_features(x, y)  # Get feature weights\n",
    "                    fts_time.append(time.time() - start_time_selection)\n",
    "                    \n",
    "                    if model == 'mcfs':\n",
    "                        ftr_selection = np.argsort(ftr_weights,0)[::-1][:n_selected_ftr]\n",
    "                    elif model == 'laplacian':\n",
    "                        ftr_selection = np.argsort(ftr_weights,0)[:n_selected_ftr]\n",
    "                    else:\n",
    "                        ftr_selection = np.argsort(ftr_weights)[::-1][:n_selected_ftr]\n",
    "                        \n",
    "\n",
    "                    # Truncate x (retain only selected features, 'remove' all others, e.g. by replacing them with 0)\n",
    "                    x_reduced = np.zeros(x.shape)\n",
    "                    x_reduced[:, ftr_selection] = x[:, ftr_selection]\n",
    "                    \n",
    "        \n",
    "                    # Test\n",
    "                    y_pred = predictor.predict(x_reduced)\n",
    "                    acc_score = accuracy_score(y, y_pred)\n",
    "                    acc.append(acc_score)\n",
    "                    \n",
    "                    # Train\n",
    "                    start_time_classification = time.time()\n",
    "                    predictor.partial_fit(x_reduced, y)\n",
    "                    clf_time.append(time.time() - start_time_classification)\n",
    "                    \n",
    "                       \n",
    "                else:\n",
    "                    fts_time.append(0)\n",
    "                    \n",
    "                    # Test\n",
    "                    y_pred = predictor.predict(x)\n",
    "                    acc_score = accuracy_score(y, y_pred)\n",
    "                    acc.append(acc_score)\n",
    "        \n",
    "                    # Train\n",
    "                    start_time_classification = time.time()\n",
    "                    predictor.partial_fit(x, y)\n",
    "                    clf_time.append(time.time() - start_time_classification)\n",
    "                \n",
    "            \n",
    "            # Average accuracy, feature selection time and training time\n",
    "            final_acc.append(np.mean(acc))\n",
    "            final_selection_time.append(np.mean(fts_time))\n",
    "            final_classification_time.append(np.mean(clf_time))\n",
    "        \n",
    "            # Restart the FileStream\n",
    "            stream.restart()\n",
    "            \n",
    "    avg_acc_score = np.mean(final_acc) \n",
    "    avg_clf_time_score = np.mean(final_classification_time) * 1000\n",
    "    avg_fts_time_score = np.mean(final_selection_time) * 1000\n",
    "    avg_time_score = avg_clf_time_score + avg_fts_time_score\n",
    "    \n",
    "    summary = {\n",
    "        'Timestamp': datetime.datetime.now(),\n",
    "        'Dataset Name': dataset_name,\n",
    "        'Model Name': model,\n",
    "        'Classifier': classifier, \n",
    "        'Accuracy': round(avg_acc_score,3),\n",
    "        'Computation Time': round(avg_time_score,1), \n",
    "        'Feature Selection Time': round(avg_fts_time_score,1), \n",
    "        'Training Time': round(avg_clf_time_score,1)\n",
    "    }\n",
    "\n",
    "    return summary\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the expirements (feature selction model and classifier)\n",
    "expirements = {\n",
    "    'FIRES_perceptron': ['perceptron', 'fires'],\n",
    "    'Max_Variance_perceptron': ['perceptron', 'max variance'],\n",
    "    'Laplacian_Score_perceptron': ['perceptron', 'laplacian'],\n",
    "    'MCFS_Score_perceptron': ['perceptron', 'mcfs'],\n",
    "    'OFFESEL_perceptron': ['perceptron', 'offesel'],\n",
    "    'Without_Feature_Selection_perceptron' : ['perceptron', 'without fs'],  \n",
    "}\n",
    "\n",
    "# Define the paths of the datasets\n",
    "datasets_info = {\n",
    "    \"HAR\":[\"datasets/har/har_iqr.csv\", -1, 2],\n",
    "    \"Spambase\":[\"datasets/spambase/spambase_iqr.csv\", -1, 1],\n",
    "    \"Gisette\":[\"datasets/gisette/gisette.csv\", -1, 5],\n",
    "    \"Madelon\":[\"datasets/madelon/madelon_iqr.csv\",-1, 5],\n",
    "    \"Dota\":[\"datasets/dota/dota.csv\", -1, 5],\n",
    "    \"KDD\":[\"datasets/kdd/kdd_iqr_binary_encoded_categorical.csv\", -1, 1],\n",
    "    \"MNIST\":[\"datasets/mnist/mnist.csv\", -1, 1],\n",
    "    \"RBF\":[\"datasets/rbf/rbf.csv\", -1, 5],\n",
    "    \"RTG\":[\"datasets/rtg/rtg.csv\", -1, 1],\n",
    "    \"Forest CoverType\":[\"datasets/covertype/covtype_iqr.csv\",-1, 1],\n",
    "    \"Poker Hand\":[\"datasets/poker/poker_hand_encoded_categorical.csv\", -1, 1],\n",
    "    \"Electricity\":[\"datasets/electricity/electricity.csv\",-1, 1],\n",
    "    \"Airlines\":[\"datasets/airlines/airlines_iqr_encoded_categorical.csv\",-1, 1],\n",
    "    \"10 KDD99\":[\"datasets/10kdd/10kdd_iqr_binary_encoded_categorical.csv\", -1, 1],\n",
    "    \"Spam Assassin Corpus\":[\"datasets/spam/spam_corpus.csv\", -1, 1],\n",
    "    \"Usenet\":[\"datasets/usenet/usenet.csv\", -1, 1],\n",
    "    \"Sensor Stream\":[\"datasets/sensor/sensor_iqr.csv\", -1, 1],\n",
    "    \"Energy Grids\":[\"datasets/energygrids/energy_grids.csv\", -1,1]\n",
    "}\n",
    "\n",
    "     \n",
    "# Define the filename of the CSV results file\n",
    "filename = 'results.csv'\n",
    "\n",
    "# Define the headers for the CSV results file\n",
    "headers = ['Timestamp', 'Dataset Name', 'Model Name', 'Classifier', 'Accuracy','Computation Time', 'Feature Selection Time', 'Training Time']\n",
    "\n",
    "# Check if the CSV results file exists\n",
    "if not os.path.exists(filename):\n",
    "    # If the file does not exist, create it with headers\n",
    "    with open(filename, 'w', encoding='UTF8', newline='') as file:\n",
    "        # Create a CSV writer object\n",
    "        dictwriter_object = DictWriter(file, fieldnames=headers)\n",
    "        # Write the headers names\n",
    "        dictwriter_object.writeheader()\n",
    "        \n",
    "    #Close the file object\n",
    "    file.close()\n",
    "    \n",
    "    \n",
    "for dataset_name, dataset_params in datasets_info.items():\n",
    "    for exp, exp_params in expirements.items():\n",
    "        \n",
    "        print('#'*50)\n",
    "\n",
    "        summary = apply_experiment(dataset_path=dataset_params[0], dataset_name=dataset_name, tgt_index=dataset_params[1], epochs=dataset_params[2], batch_sizes=[25, 50, 75, 100], fractions=[0.1, 0.15, 0.2], classifier=exp_params[0], model=exp_params[1])\n",
    "        \n",
    "        # Print the summary values\n",
    "        result_string = f\"{summary['Timestamp']} - Dataset: {summary['Dataset Name']}, Model: {summary['Model Name']}, Accuracy: {summary['Accuracy']}, Computation Time: {summary['Computation Time']}, Feature Selection Time: {summary['Feature Selection Time']}, Training Time: {summary['Training Time']}\"\n",
    "        \n",
    "        print(result_string)\n",
    "        \n",
    "        with open(filename, 'a', encoding='UTF8', newline='') as file:\n",
    "            # Create a CSV writer object\n",
    "            dictwriter_object = DictWriter(file, fieldnames=headers)\n",
    "            # Write a new row of data to the CSV file\n",
    "            dictwriter_object.writerow(summary)\n",
    "  \n",
    "        #Close the file object\n",
    "        file.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FIRES.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (fires-env)",
   "language": "python",
   "name": "fires-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
